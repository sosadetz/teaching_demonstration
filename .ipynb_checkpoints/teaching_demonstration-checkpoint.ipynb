{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use Ngrams to Solve Problems in the Humanities\n",
    "\n",
    "### 1. An Introduction to Google's Ngram Viewer\n",
    "\n",
    "You can think of an ngram like a word or phrase. In computational linguistics, ngrams are just set sequences of items (usually words) that appear together. In _The Wizard of Oz_, when Dorothy says \"Toto, I have a feeling we are not in Kansas anymore,\" she's produced an utterance with the following:\n",
    "\n",
    ">__11__ 1-grams (or unigrams, the words themselves);\n",
    "\n",
    ">__10__ 2-grams (or bigrams, such as \"Toto I\", \"I have\", \"have a\", etc.); and,\n",
    "   \n",
    "Here. Before we move on, try it for yourself.\n",
    "\n",
    "How many unigrams are there in Hamlet's **\"To be or not to be, that is the question\"**? How many discrete bigrams?\n",
    "\n",
    "\n",
    "### 2. Let's examine a few simple ngram searches:\n",
    "\n",
    "<a href='https://books.google.com/ngrams/graph?content=civil+rights&year_start=1800&year_end=2008&corpus=15&smoothing=3&share=&direct_url=t1%3B%2Ccivil%20rights%3B%2Cc0'>\"Civil Rights\"</a>\n",
    "\n",
    "This search uses wildcards and parts of speech:\n",
    "<a href='https://books.google.com/ngrams/graph?content=epic_NOUN%2Csatire_NOUN%2Clyric_NOUN%2Code_NOUN%2Celegy_NOUN%2Csonnet_NOUN&year_start=1700&year_end=2008&corpus=15&smoothing=3&share=&direct_url=t1%3B%2Cepic_NOUN%3B%2Cc0%3B.t1%3B%2Csatire_NOUN%3B%2Cc0%3B.t1%3B%2Clyric_NOUN%3B%2Cc0%3B.t1%3B%2Code_NOUN%3B%2Cc0%3B.t1%3B%2Celegy_NOUN%3B%2Cc0%3B.t1%3B%2Csonnet_NOUN%3B%2Cc0'>Genres of Poetry</a>\n",
    "\n",
    "__<p style=\"text-align: center;\">Now, spend five minutes trying out different searches on your own.</p>__\n",
    "__<p style=\"text-align: center;\">What were you able to find?</p>__\n",
    "\n",
    "\n",
    "### 3. Let's now turn to some problems with the Ngram Viewer\n",
    "\n",
    "For instance, can you see what's the matter with this search? <a href='https://books.google.com/ngrams/graph?content=foil&year_start=1700&year_end=2008&corpus=15&smoothing=3&share=&direct_url=t1%3B%2Cfoil%3B%2Cc0'>foil</a>\n",
    "\n",
    "\n",
    "What's going on here? <a href='https://books.google.com/ngrams/graph?content=Figure%2Cfigure&year_start=1800&year_end=2008&corpus=15&smoothing=3&share=&direct_url=t1%3B%2CFigure%3B%2Cc0%3B.t1%3B%2Cfigure%3B%2Cc0'>Figure</a>\n",
    "\n",
    "Finally, what about this case? <a href='https://books.google.com/ngrams/graph?content=Bilbo%2CGandalf%2CGollum%2CSmaug&year_start=1900&year_end=2008&corpus=15&smoothing=0&share=&direct_url=t1%3B%2CBilbo%3B%2Cc0%3B.t1%3B%2CGandalf%3B%2Cc0%3B.t1%3B%2CGollum%3B%2Cc0%3B.t1%3B%2CSmaug%3B%2Cc0'>Characters in The Hobbit</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngrams Revisited (A Quick Intro to Teaching Students to Code with Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By learning to code in Python with Jupyter Notebooks, we can write scripts that answer much more sophisticated questions. Before we get started, let's take a look at how a Jupyter Notebook works. Notebooks are composed of `markdown` (or text) cells in white, and `coding` cells in grey. You can modify the code in a coding cell, and then press `Shift` + `Enter` to run that snippet of code, saying \"`Hello, World!`\", and then modify it to say something else, like \"`Let's start coding!`\" Whenever you run a coding cell, that small empty circle in top, right-hand corner of this window will temporarily turn black, telling you that your computer is chugging through your script. Usually, this should just take a few seconds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello, World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each coding cell actually runs the code it contains, as we can see when we run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2 + 2\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we're going to do a number of things that the Google Ngram Viewer can't. __Our central question is this: can simple counts of unigrams and bigrams tell us important things about particular literary works?__ Our goal is to produce an interactive graph that looks like this:\n",
    "\n",
    "<img src=\"collected_corpora_word_and_bigram_variance.png\">\n",
    "\n",
    "It might look like a long lesson, but in reality it's just five simple operations. Here's what we'll do:\n",
    "\n",
    "1. Graph the most common words in _The Prelude_;\n",
    "2. Graph the most common bigrams in _The Prelude_;\n",
    "3. Read all the books in a single file into a dictionary;\n",
    "4. Count and graph one word across that corpus;\n",
    "5. Count (and finally, visualize) a whole set of statistical measures for a corpus.\n",
    "\n",
    "With each new operation, we'll introduce a few small tweaks that will make the script do something fundamentally different, but the structure of each operation will remain largely the same. First, we'll open a file or a set of files, then, we'll perform an operation on them, and finally, we'll get some kind of output.\n",
    "\n",
    "To do this, we'll be using a number of curated corpora:\n",
    "\n",
    "1. __romantic_literature__ -> a set of works by British Romantic authors\n",
    "2. __popular_literature__ -> a small set of popular children's series, including _Harry Potter_, _The Lord of the Rings_, and _Dune_\n",
    "3. __collected_literature__ -> an even larger set that includes those listed above, as well as a number of eighteenth-century works and lyrics by various hip hop artists.\n",
    "\n",
    "\n",
    "\n",
    "In a normal class, we would spend some time discussing the importance of preprocessing text and metadata. For this demonstration, I've  done all the processing in advance, so that each set exists in a number of states of preparation. This will allow us to quickly grab the most appropriate documents, and significantly decrease the time we spend waiting for code to run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Graph the frequencies of the most common words in _The Prelude_, by William Wordsworth\n",
    "\n",
    "Let's start by opening the file for _The Prelude_ and reading it. Just like a physical book, in order to do things with a file computationally, you have to open the file as a <b>file object</b> and read the bytes that it contains.\n",
    "\n",
    "In this case, we'll be using a version of _The Prelude_ that's been modified a bit. Its <b>stop words</b> (\"the\", \"a\", \"and\", etc.) have been removed, but it hasn't been <b>stemmed</b>, which means that `beauty` and `beauties` are still different words, and they haven't been collapsed into the same word `beauti`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = open(\"romantic_literature/romantic_literature_clean_nostems_nostops/wordsworth_prelude.txt\", \"r\")\n",
    "doc1Txt = doc1.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does that file look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc1Txt[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 250 items in the file are all discrete characters. In order to recognize the words as discrete items, we have to tokenize the file. From time to time, we'll download a <b>library</b>. Libraries are collections of pre-written code that simplify writing code, and they're much of the reason why Python is such a powerful and appealing language. Here, we'll use a tokenizer called `punkt` to separate _The Prelude_ into distinct words. `punkt` is part of the `Natural Langauge Toolkit`, or `NLTK`, a massive collection of scripts that humanists working in DH use all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "words = nltk.tokenize.word_tokenize(doc1Txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've turned _The Prelude_ into a massive list of 27,253 words. By running the following two cells, let's make a chart of the words in the book, ordered by their frequency, and then graph the top 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "import pandas as pd\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "fdist_words = FreqDist(words)\n",
    "count_frame_words = pd.DataFrame(fdist_words, index =['word_count']).T\n",
    "count_frame_words = count_frame_words.sort_values('word_count', ascending=False)\n",
    "display(HTML(count_frame_words.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.gca()    \n",
    "count_frame_words['word_count'][:60].plot(kind = 'bar', ax=ax, color='teal')\n",
    "ax.set_title('Frequency of the most common words')\n",
    "ax.set_ylabel('Frequency of word')\n",
    "ax.set_xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already, this graph tells us a lot about Wordsworth's poem. Does it accord with your experience of the poem? Students might be asked to think about how these words reflect the subject matter of the poem, its concern with nature and human psychology is evident, but what might not be apparent is the lack of any proper nouns in these top words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Graph the most common ngrams for _The Prelude_\n",
    "\n",
    "Now that we've graphed the most common words, it's a simple matter to modify our script to count and graph the most common Ngrams. Before we do so, though, we have to change the file that we're using, because in this case we require stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = open(\"romantic_literature/romantic_literature_clean/wordsworth_prelude.txt\", \"r\")\n",
    "doc2Txt = doc2.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2 = nltk.tokenize.word_tokenize(doc2Txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we'll be using the `NLTK` library `ngrams`. That first block of code is all that's different. It simply cycles over every two words, making a list of the bigrams, as they occur. Want to try trigrams? Change `n = 3` and re-run the script. It's that simple.\n",
    "\n",
    "(Pro tip!: If you change the main variable so that `n = 1`, you'll get a graph of unigrams, but this time with stop words included.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "# Get n-grams where n = 2\n",
    "n = 2\n",
    "nGramsInDoc = []\n",
    "nGrams = ngrams(words2, n)\n",
    "for grams in nGrams:\n",
    "    nWords = ' '.join(g for g in grams)\n",
    "    nGramsInDoc.append(nWords)\n",
    "\n",
    "# Count the frequency of each n-gram\n",
    "fdist = FreqDist(nGramsInDoc)\n",
    "count_frame = pd.DataFrame(fdist, index =[0]).T\n",
    "count_frame.columns = ['Count']\n",
    "\n",
    "# Plot the frequency of the top 60 bigrams\n",
    "counts = count_frame.sort_values('Count', ascending = False)\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.gca()    \n",
    "counts['Count'][:60].plot(kind = 'bar', ax = ax, color='teal')\n",
    "ax.set_title('Frequency of the most common n-grams')\n",
    "ax.set_ylabel('Frequency of n-gram')\n",
    "ax.set_xlabel('n-gram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add all the books in a folder to a dictionary\n",
    "\n",
    "Now that we're becoming comfortable with unigrams and bigrams, let's practice working with a whole corpus of texts. The modules `os` and `glob`, which we'll be using in this case, allow us to use operating system functonality and identify a set of pathnames in a directory. This is a quick way to get a list of files, which we'll use in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"romantic_literature/romantic_literature_clean_nostems/\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "filenames = glob.glob(\"*.txt\")\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of the files in our directory, we just have to open and read them, as we did before. In this case, we'll make each file a value in a `dictionary`. Here, we'll be using a `loop`, an important coding concept that allows our code to iterate over each file in a directory. We used a loop in the last section, when we calculated Ngrams, but it was so brief that I passed over it. From now on, beacuse we'll be working with multiple files, loops will become a lot more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dictionary = {}\n",
    "\n",
    "for file in filenames:\n",
    "    with open(str(file), 'r') as inputFile:\n",
    "        readFile = inputFile.read()\n",
    "        text_dictionary[str(file).format(file)] = readFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the identifying key for each book is just the name of its file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dictionary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dictionary['wordsworth_prelude.txt'][:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Count and graph one word across all the books in a corpus\n",
    "\n",
    "Let's say you wanted to study the importance of a particular word across a number of books. We can start to do this by simply counting the number of time it occurs in a particular work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_word = \"nature\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll use the `split` method in much the same way as we used the `punkt` tokenizer earlier. We'll then use the `counter` library to count each time the token `nature` occurs. \n",
    "\n",
    "(Keep in mind that the texts we're examining haven't been `stemmed`, so `nature` and `natures` and `natural` are recognized as different tokens!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for key in text_dictionary:\n",
    "    word_counts = Counter(text_dictionary[key].split())\n",
    "    print(key, \"-> \" + search_word + \" =\", word_counts[search_word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks as though `nature` is especially common in Coleridge's _Biographia Literaria_ and Mary Shelley's _The Last Man_. Let's use the following cells to print out a quick graph of its raw count for every text in our `romantic_literature` set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_df = pd.DataFrame(columns=[\"Count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in text_dictionary:\n",
    "    search_df.loc[key] = Counter(text_dictionary[key].split())[search_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the frequency of the searchword\n",
    "counts = search_df.sort_values('Count', ascending = False)\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.gca()    \n",
    "counts['Count'].plot(kind = 'bar', ax = ax, color='teal')\n",
    "ax.set_title('Frequency of the term ' + search_word)\n",
    "ax.set_ylabel('Frequency of ' + search_word)\n",
    "ax.set_xlabel('text')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Get word count, unique words, variance, unique ngrams, and ngram variance for all books in directory (based on abridged set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple raw counts of a particular word are fine, but there's an obvious problem: documents are different lengths. By introducing a few new lines of code, we can get the length of each file in a directory, its raw count of unigrams and bigrams, as well as their variance. Variance is just the raw count divided by the length of the volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/jovyan/collected_corpora/collected_corpora_keatsabbr/\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob.glob(\"*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dictionary = {}\n",
    "\n",
    "for file in filenames:\n",
    "    with open(str(file), 'r') as inputFile:\n",
    "        readFile = inputFile.read()\n",
    "        text_dictionary[str(file).format(file)] = readFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "appended_data = []\n",
    "n = 2\n",
    "\n",
    "for text in text_dictionary:\n",
    "        words = nltk.tokenize.word_tokenize(text_dictionary[text])\n",
    "        total_count = len(words)\n",
    "        discrete_count = len(list(dict.fromkeys(words)))\n",
    "\n",
    "        ngrams_counter = collections.Counter()\n",
    "        n_grams = nltk.ngrams(words, n)\n",
    "        ngrams_counter.update(n_grams)\n",
    "        ngrams_count = len(ngrams_counter)\n",
    "        \n",
    "        variance = str(discrete_count/total_count)\n",
    "        ngrams_variance = str(ngrams_count/total_count)\n",
    "        \n",
    "        data = {'DocName':text, 'total_words':total_count, 'discrete_count':discrete_count, 'variance':variance, 'ngrams_count': ngrams_count, 'ngrams_variance': ngrams_variance}\n",
    "        appended_data.append(data)\n",
    "\n",
    "appended_data_df = pd.DataFrame(appended_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we're going to produce some data that's pretty startling. Normally, we'd write out our results to a `csv file`, which is like a simple type of spreadsheet. That's a bit complicated for the purposes of this demonstration, because it might be complicated to try to write files to your different computers. Instead, let's just print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(HTML(appended_data_df.to_html()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've got the data, you're welcome to download Tableau and manipulate it on your own. One of the nice things about Tableau is that it allows you to save your graph as a url, so that to see the results, all you have to do is click on the link below.\n",
    "\n",
    "In this graph, we'll be examining the relationship between overall word (unigram) variance and bigram variance. \n",
    "\n",
    "https://public.tableau.com/profile/stephen8554#!/vizhome/collected_corpora_simple_counting_results/Sheet1?publish=yes\n",
    "\n",
    "Take a moment and try clicking on the various \"groups\" in the top right corner of the chart. You'll see that the data tends to cluster in interesting ways.\n",
    "\n",
    "  __Discussion Questions__\n",
    "\n",
    "1. Do any of the artists who exhibit particularly low or high variance - for words _or_ ngrams - suprise you? Why?\n",
    "\n",
    "2. Although the different groups cluster together, each group is fairly heterogenous within itself. Can you identity even more granular groups of authorsthat cluster together?\n",
    "    \n",
    "3. Do you think there is a correlation between word or bigram variance and literary quality?\n",
    "    \n",
    "4. This graph compares apples and oranges, somewhat, because the 18c, Romantic, and popular sets are often made of a coherent excerpt from a published work, whereas the files in the hip hop set are miscellaneous collections of artists' songs. Could that be distorting the data you see in any way?\n",
    "    \n",
    "5. What do you think might happen if you change the analysis to look for trigram variance? Try for yourself, and see if you were right."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
